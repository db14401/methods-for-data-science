{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNqIbKuXY0OV"
   },
   "source": [
    "# Error Backpropagation / Convolutional Neural Networks\n",
    "\n",
    "The purpose of this notebook is to practice implementing the error backpropagation algorithm and the convolutional layers of CNN models.\n",
    "\n",
    "We will implement the backpropagation algorithm and a 1-D convolution in numpy, and use TensorFlow to train a CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ii9JKVDtY0On"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq2bV416Y0Oq"
   },
   "source": [
    "## 1. Error Backpropagation\n",
    "\n",
    "In this section we will practice implementing the backpropagation algorithm to compute gradients for an MLP with a single hidden layer, and train the model by applying the computed gradients with a network optimiser.\n",
    "\n",
    "For this tutorial we will use the [QSAR fish toxicity dataset](https://archive.ics.uci.edu/ml/datasets/QSAR+fish+toxicity) from the UCI machine learning repository.\n",
    "\n",
    "We will start by loading this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QA_og_q_ZEjH"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17976/2553158294.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mupload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "upload = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V_NLgeuaY0Ot"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'qsar_fish_toxicity.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17976/2997390840.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the data from CSV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'qsar_fish_toxicity.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m## <-- inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m## <-- targets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoding_errors\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         )\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m             )\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'qsar_fish_toxicity.csv'"
     ]
    }
   ],
   "source": [
    "# Load the data from CSV\n",
    "\n",
    "data = pd.read_csv('qsar_fish_toxicity.csv', sep=';', header=None)\n",
    "x = data.iloc[:, :6]  ## <-- inputs\n",
    "y = data.iloc[:, 6:]  ## <-- targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "K-A-IUOqY0Ov"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17976/4155543126.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# Create training and validation splits\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Zv7-vgoY0Ov"
   },
   "outputs": [],
   "source": [
    "# Standardise the inputs\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mZIM91XGY0Ox",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17976/969908508.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the data into Dataset objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1202\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the data into Dataset objects\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32), y_train.astype(np.float32)))\n",
    "train_data = train_data.shuffle(1202)\n",
    "train_data = train_data.batch(128)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test.astype(np.float32), y_test.astype(np.float32)))\n",
    "test_data = test_data.batch(128)\n",
    "\n",
    "train_data.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuVi05qpY0Oy"
   },
   "source": [
    "We will use an MLP model with a single hidden layer with 64 neurons, and a tanh activation function. The output layer has 1 neuron and no activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DQXHMF-FY0Oz",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                448       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 513\n",
      "Trainable params: 513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='tanh', input_shape=(6,)),\n",
    "    Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gI46w68TY0O3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17976/656870749.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse')\n",
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkH-4rwqY0O5"
   },
   "source": [
    "This MLP model has two kernel variables and two bias variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "e38Z1it7Y0O8"
   },
   "outputs": [],
   "source": [
    "W0 = model.layers[0].kernel\n",
    "b0 = model.layers[0].bias\n",
    "\n",
    "\n",
    "W1 = model.layers[1].kernel\n",
    "b1 = model.layers[1].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLxSWFKsY0O8"
   },
   "source": [
    "In this tutorial we will implement the forward and backward pass of backpropagation manually.\n",
    "\n",
    "The following function implements the dense layer transformation to obtain the pre-activation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_XfRb61Y0O9"
   },
   "outputs": [],
   "source": [
    "def dense(h, W, b):\n",
    "    \n",
    "    # h: K x h_in array of inputs\n",
    "    # W: h_in x h_out array for kernel matrix parameters\n",
    "    # b: Length h_out 1-D array for bias parameters\n",
    "    # returns: K x h_out output array \n",
    "    \n",
    "    return b + h @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1qwuIxMY0O-"
   },
   "source": [
    "Recall that the backpropagation algorithm can be summarised as follows:\n",
    "\n",
    ">1. Define the **error** $\\delta^{(k)}_p := \\frac{\\partial \\mathcal{L}_i}{\\partial a^{(k)}_p}$ for layer $k$, where we denote $\\mathcal{L}_i$ as the loss for example $i$, and $a^{(k)}_p$ is the $p$-th pre-activation in layer $k$\n",
    ">2. Propagate the signal forwards by passing an input vector $x_i$ through the network and computing all pre-activations and post-activations using $\\mathbf{a}^{(k)} = (\\boldsymbol{h}^{(k-1)})^T\\boldsymbol{W}^{(k-1)} + \\boldsymbol{b}^{(k-1)}$\n",
    "> 3. Evaluate $\\boldsymbol{\\delta}^{(L+1)} = \\frac{\\partial \\mathcal{L}_i}{\\partial \\boldsymbol{a}^{(L+1)}}$ for the output neurons\n",
    "> 4. Backpropagate the errors to compute $\\mathbf{\\delta}^{(k)}$ for each hidden unit using $\\boldsymbol{\\delta}^{(k)} = \\boldsymbol{\\sigma}'(\\boldsymbol{a}^{(k)})  \\boldsymbol{W}^{(k)} \\boldsymbol{\\delta}^{(k+1)}$\n",
    "> 5. Obtain the derivatives of $\\mathcal{L}_i$ with respect to the weights and biases using $\\frac{\\partial \\mathcal{L}_i}{\\partial w^{(k)}_{pq}} = \\delta^{(k+1)}_p h^{(k)}_q,\\quad \n",
    "\\frac{\\partial \\mathcal{L}_i}{\\partial b^{(k)}_{p}} = \\delta^{(k+1)}_p$\n",
    "\n",
    "In the above, $\\boldsymbol{\\sigma}'(\\boldsymbol{a}^{(k)})$ is a diagonal matrix with diagonal elements $\\sigma'(a^{(k)}_p)$, $p=1,\\ldots,n_k$, where $n_k$ is the number of neurons in hidden layer $k$.\n",
    "\n",
    "In our case, we have an MLP with one hidden layer ($L=1$). The loss $\\mathcal{L}_i$ for data example $(x_i, y_i)$ will be the mean squared error\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_i = \\frac{1}{2}(y_i - f(x_i))^2,\n",
    "$$\n",
    "\n",
    "where $f:\\mathbb{R}^6\\mapsto\\mathbb{R}$ is the MLP network.\n",
    "\n",
    "We will start by computing the output error $\\boldsymbol{\\delta}^{(2)} = \\frac{\\partial \\mathcal{L}_i}{\\partial \\boldsymbol{a}^{(2)}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFwEGliHY0O_"
   },
   "outputs": [],
   "source": [
    "## EDIT THIS FUNCTION\n",
    "def output_error(y_batch, a2):\n",
    "    \n",
    "    # y_batch: K x 1 array of data outputs\n",
    "    # a2: K x 1 array of output pre-activations\n",
    "    # returns: K x 1 array of output errors \n",
    "    \n",
    "    return a2 - y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcW_g3wTY0O_"
   },
   "source": [
    "We will also need to compute the diagonal matrix $\\boldsymbol{\\sigma}'(\\boldsymbol{a}^{(1)})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_cLVXerY0O_"
   },
   "outputs": [],
   "source": [
    "## EDIT THIS FUNCTION\n",
    "def activation_derivative(a1):\n",
    "    \n",
    "    # a1: K x 64 array of hidden layer pre-activations\n",
    "    # returns: K x 64 array of diagonal elements  \n",
    "    \n",
    "    return 1 - np.square(np.tanh(a1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXeci9rPY0PA"
   },
   "source": [
    "Given the output error $\\boldsymbol{\\delta}^{(2)}$, kernel matrix $\\boldsymbol{W}^{(1)}$ and the preactivations $\\boldsymbol a^{(1)}$, we need to backpropagate the error $\\boldsymbol{\\delta}^{(2)}$ to get $\\boldsymbol{\\delta}^{(1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grs6OPjoY0PB"
   },
   "outputs": [],
   "source": [
    "## EDIT THIS FUNCTION\n",
    "def backpropagate(delta2, W1, a1):\n",
    "    \n",
    "    # delta2: K x 1 array of output errors\n",
    "    # W1: 64 x 1 array\n",
    "    # a1: K x 64 array of hidden layer pre-activations\n",
    "    # returns: K x 64 array of hidden layer errors\n",
    "    \n",
    "    return np.squeeze(activation_derivative(a1)[..., np.newaxis] * W1 * delta2[:, np.newaxis, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnQeU133Y0PB"
   },
   "source": [
    "Finally, given the errors $\\boldsymbol{\\delta}^{(1)}$ and $\\boldsymbol{\\delta}^{(2)}$ and post-activations $\\boldsymbol{h}^{(0)}$ ($=\\boldsymbol{x}$) and $\\boldsymbol{h}^{(1)}$, we can compute the gradients $\\frac{\\partial\\mathcal{L}_i}{\\partial \\boldsymbol{W}^{(0)}}$, $\\frac{\\partial\\mathcal{L}_i}{\\partial \\boldsymbol{b}^{(0)}}$, $\\frac{\\partial\\mathcal{L}_i}{\\partial \\boldsymbol{W}^{(1)}}$ and $\\frac{\\partial\\mathcal{L}_i}{\\partial \\boldsymbol{b}^{(1)}}$.\n",
    "\n",
    "The function below should compute these gradients, averaged over the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5tSWH7mY0PB"
   },
   "outputs": [],
   "source": [
    "## EDIT THIS FUNCTION\n",
    "def grads(delta1, delta2, h0, h1):\n",
    "    \n",
    "    # delta1: K x 64 array of hidden layer errors\n",
    "    # delta2: K x 1 array of output errors\n",
    "    # h0: K x 6 array of inputs\n",
    "    # h1: K x 64 array of hidden layer post-activations\n",
    "    # returns: tuple of arrays of shape (6 x 64), (64,), (64 x 1), (1,) for gradients\n",
    "    \n",
    "    grad_W0 = delta1[:, np.newaxis, :] * h0[:, :, np.newaxis]\n",
    "    grad_b0 = delta1\n",
    "    grad_W1 = delta2[:, np.newaxis, :] * h1[:, :, np.newaxis]\n",
    "    grad_b1 = delta2\n",
    "    \n",
    "    grad_W0 = tf.reduce_mean(grad_W0, axis=0)\n",
    "    grad_b0 = tf.reduce_mean(grad_b0, axis=0)\n",
    "    grad_W1 = tf.reduce_mean(grad_W1, axis=0)\n",
    "    grad_b1 = tf.reduce_mean(grad_b1, axis=0)\n",
    "    \n",
    "    return grad_W0, grad_b0, grad_W1, grad_b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP5L-G-LY0PC"
   },
   "source": [
    "We now have what we need to compute the gradients in order to train the model using a network optimiser.\n",
    "\n",
    "The code below will run the training loop using your functions above, and apply the gradients using an RMSprop optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsUxRHY1Y0PD"
   },
   "outputs": [],
   "source": [
    "# Run the training loop\n",
    "\n",
    "epochs = 100\n",
    "best_val_loss = np.inf\n",
    "rmsprop = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x_batch, y_batch in train_data:\n",
    "        losses.append(tf.reduce_mean(tf.keras.losses.mse(y_batch, model(x_batch))).numpy())\n",
    "        \n",
    "        a1 = dense(x_batch, W0, b0)\n",
    "        h1 = np.tanh(a1)\n",
    "        a2 = dense(h1, W1, b1)\n",
    "        \n",
    "        delta2 = output_error(y_batch, a2)\n",
    "        delta1 = backpropagate(delta2, W1, a1)\n",
    "        \n",
    "        var_grads = grads(delta1, delta2, x_batch, h1)\n",
    "        \n",
    "        rmsprop.apply_gradients(zip(var_grads, [W0, b0, W1, b1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K08rhwJJY0PD"
   },
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wJgPAooY0PF"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "\n",
    "model.compile(loss='mse')\n",
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X0OteAPY0PG"
   },
   "source": [
    "#### Questions\n",
    "1. How would the backpropagation algorithm be modified for a different loss function, e.g. binary cross entropy loss, or a softmax function?\n",
    "2. What difference would it make if we used a ReLU activation function or a sigmoid activation instead? \n",
    "3. Under what circumstances can the gradients be diminished or exploded, especially in the earlier layers of a deep network model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3RNg0LRY0PH"
   },
   "source": [
    "## 2. Convolutional Neural Networks\n",
    "\n",
    "In this section we will practice implementing a 1-dimensional convolutional operation in numpy. \n",
    "\n",
    "We will then train a CNN model with TensorFlow.\n",
    "\n",
    "For this section you will be working with the [Human Activity Recognition (HAR) Using Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) dataset. This consists of the readings from an accelerometer (which measures acceleration) carried by a human doing different activities. The six activities are walking horizontally, walking upstairs, walking downstairs, sitting, standing and laying down. The accelerometer is inside a smartphone, and, every 0.02 seconds, it takes six readings: linear and gyroscopic acceleration in the x, y and z directions. \n",
    "\n",
    "The goal is to use the accelerometer data to predict the activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "e5qjIoByY0PI"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'x_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17976/3539838589.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x_train.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'y_train.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'x_train.npy'"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "\n",
    "x_val = np.load('x_val.npy')\n",
    "y_val = np.load('y_val.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_7_7QmLY0PJ"
   },
   "source": [
    "The input data consists of 6 features over 128 time steps, and the output data is a single integer from 0 to 5, which denotes the class.\n",
    "\n",
    "There are 7,352 examples in the training set and 2,947 examples in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kq1ijauTY0PK"
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GXUtuo0xY0PK"
   },
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'Walking',\n",
    "    'Walking upstairs',\n",
    "    'Walking downstairs',\n",
    "    'Sitting',\n",
    "    'Standing',\n",
    "    'Laying'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "HlDpeL3WY0PL"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17976/103949204.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0minx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mx_example\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot a randomly selected example from each class\n",
    "\n",
    "for l, label in enumerate(range(6)):\n",
    "    inx = np.where(y_train[:, 0] == label)[0]\n",
    "    i = np.random.choice(inx)\n",
    "    x_example = x_train[i]\n",
    "    fig, ax = plt.subplots(figsize=(10, 1))\n",
    "    ax.imshow(x_example.T, cmap='Greys', vmin=-1, vmax=1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(classes[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yXkPtG6Y0PL"
   },
   "source": [
    "We will train a 1-D CNN classifier model on this dataset.\n",
    "\n",
    "Now you should implement a 1-D convolutional layer in numpy. The following function should implement the convolutional transformation of this layer, given kernel and bias parameters and the input to the layer. The layer should use \"VALID\" padding and a stride of one. It should return the layer pre-activations (no activation function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uk33QUuQY0PM"
   },
   "outputs": [],
   "source": [
    "## EDIT THIS FUNCTION\n",
    "def conv1d(x, K, b):\n",
    "    \n",
    "    # x: K x w_in x c_in array of inputs\n",
    "    # K: k_w x c_in x c_out array for kernel parameters\n",
    "    # b: Length c_out 1-D array for bias parameters\n",
    "    # returns: K x w_out x c_out output array \n",
    "    \n",
    "    \n",
    "    batch_size = x.shape[0]\n",
    "    w_in = x.shape[1]\n",
    "    k_w = K.shape[0]\n",
    "    c_out = K.shape[2]\n",
    "    w_out = w_in - k_w + 1\n",
    "    outputs = np.zeros((batch_size, w_out, c_out))\n",
    "    \n",
    "    for i in range(w_out):\n",
    "        outputs[:, i, :] = (x[:, i: i + k_w, :, np.newaxis] * K).sum(axis=(1, 2)) + b ## <-- pre-activations\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dshIXABmY0PN",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Conv1D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17976/1656860297.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Test your layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mconv_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Conv1D' is not defined"
     ]
    }
   ],
   "source": [
    "# Test your layer\n",
    "\n",
    "conv_layer = Conv1D(8, 16, activation=None)\n",
    "inputs = tf.random.normal((16, 128, 6))\n",
    "y_tf = conv_layer(inputs)\n",
    "\n",
    "y = conv1d(inputs.numpy(), conv_layer.kernel.numpy(), conv_layer.bias.numpy())\n",
    "np.allclose(y, y_tf.numpy(), atol=1e-4)  ## <-- should be True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFoo0QAmY0PN"
   },
   "source": [
    "You should now build the CNN model in TensorFlow to train on the HAR dataset. This model should consist of:\n",
    "\n",
    "* A `Conv1D` layer with 8 filters, kernel width of 16 and a ReLU activation\n",
    "  * The input shape of this first layer should be set to `(128, 6)`\n",
    "  * This layer should use l2 kernel weight regularisation with a coefficient of 1e-3\n",
    "* A `MaxPooling1D` layer with a pooling window size of 16\n",
    "* A `Flatten` layer\n",
    "* A `Dense` layer with 6 neurons and a softmax activation\n",
    "  * This layer should use l2 kernel weight regularisation with a coefficient of 1e-3\n",
    "\n",
    "The function below should build and compile this model, using an Adam optimizer and a categorical accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xbSw8DyY0PO"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "## EDIT THIS FUNCTION\n",
    "def get_model():\n",
    "    model = Sequential([\n",
    "        Conv1D(8, 16, activation='relu', kernel_regularizer=l2(1e-3), input_shape=(128, 6)),\n",
    "        MaxPooling1D(16),\n",
    "        Flatten(),\n",
    "        Dense(6, kernel_regularizer=l2(1e-3), activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDMUuuScY0PP"
   },
   "outputs": [],
   "source": [
    "# Run your function to get the model and print the summary\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_v_BDBHSY0PP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data into Dataset objects\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32), y_train.astype(np.float32)))\n",
    "train_data = train_data.shuffle(1024)\n",
    "train_data = train_data.batch(128)\n",
    "\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_val.astype(np.float32), y_val.astype(np.float32)))\n",
    "val_data = val_data.batch(128)\n",
    "\n",
    "train_data.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "411MNlZxY0PP"
   },
   "source": [
    "Now we are ready to train the model. The following function should run the training for a maximum of 200 epochs, validating the model with early stopping that monitors the validation accuracy, and using a patience of 15 epochs. The function should return the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3jTqRMSY0PQ"
   },
   "outputs": [],
   "source": [
    "# Train the model, using early stopping\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def train_model(model):\n",
    "    early_stopping = EarlyStopping(patience=15, monitor='val_accuracy')\n",
    "    history = model.fit(train_data, epochs=200, verbose=0, validation_data=val_data,\n",
    "                        callbacks=[early_stopping])\n",
    "    return history\n",
    "\n",
    "history = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YC7k96O0Y0PQ"
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "fig.add_subplot(121)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.title(\"Loss vs epoch\")\n",
    "\n",
    "fig.add_subplot(122)\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Categorical accuracy\")\n",
    "plt.title(\"Accuracy vs epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFcmXjsFY0PS"
   },
   "source": [
    "Now let's take a look at some model predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhv2uaVGY0PS"
   },
   "outputs": [],
   "source": [
    "# Get the model predictions\n",
    "\n",
    "preds = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYnYyrN4Y0PS",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot some example predictions\n",
    "\n",
    "num_preds = preds.shape[0]\n",
    "num_examples = 10\n",
    "inx = np.random.choice(num_preds, num_examples, replace=False)\n",
    "gs = {\"width_ratios\": [2, 1]}\n",
    "\n",
    "for i in inx:\n",
    "    x_example = x_val[i]\n",
    "    true_label = y_val[i][0]\n",
    "    prediction = preds[i]\n",
    "    pred_class = np.argmax(prediction)\n",
    "    fig, ax = plt.subplots(figsize=(14, 1), ncols=2, gridspec_kw=gs)\n",
    "    ax[0].imshow(x_example.T, cmap='Greys', vmin=-1, vmax=1)\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title(\"True label: {}\\nPredicted label: {}\".format(classes[true_label], classes[pred_class]))\n",
    "    ax[1].bar(['Walk', 'Walk\\nUp', 'Walk\\nDown', 'Sit', 'Stand', 'Lay'], prediction)\n",
    "    ax[1].set_title(\"Class predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66bHzMiGY0PS"
   },
   "source": [
    "#### Questions\n",
    "1. Are there particular classes where the model tends to be more uncertain in its predictions?\n",
    "2. What affect does the weight regularisation have on the training and the final model? Try experimenting with no weight regularisation, and different values of the regularisation coefficient.\n",
    "3. How did the patience hyperparameter impact the training run above? What would have happened if we set patience to zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "b = a*1\n",
    "print(2*b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2 - Backprop & ConvNets solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
